# 数据同步：主从库如何实现数据一致？

前面学习了RDB和AOF，如果Redis发生了宕机，可以分别采用回放日志和重新读入RDB文件的方式来恢复数据，从而尽量少丢失数据，提高可靠性。

但是即使使用了这两种方法，也可能存在服务不可用的问题。比如说，实际运行时只运行了一个Redis实例，那么如果这个实例宕机了，它在恢复期间也是无法服务新来的数据存取请求的。

那么平常所说的Redis具有高可靠性，其实有两层含义，一是**数据尽量少丢失**，二是**服务尽量少中断**。AOF和RDB保证了前者，而对于后者，Redis的做法就是**增加副本冗余量**，将一份数据同时保存在多个实例上，即使有一个实例出现了故障，需要过一段时间才能恢复，其他实例也可以对外提供服务，不会影响业务使用。

多个实例保存同一份数据，听起来不错，但是需要考虑一个问题：这么多副本，它们之间的数据如何保持一致呢？数据读写操作可以发送给所有的实例吗？

实际上，Redis提供了主从库模式，以保证数据副本的一致，主从库之间采用的是`读写分离`的模式。

**读操作**：主库、从库都可以接收；

**写操作**：首先到主库执行，然后主库将操作同步给从库。

![image-20250224162518351](https://lyb-1305354270.cos.ap-beijing.myqcloud.com/image-20250224162518351.png)

主从库采用读写分离后，所有数据的修改都只在主库上完成，不用协调三个实例。而主库有了最新的数据之后，会同步给从库，这样主从库的数据就是一致的。

下面就了解一下主从同步的原理以及应对网络断连风险的方案：

### 主从库如何进行第一次同步？

当我们启动多个Redis实例的时候，它们互相之间就可以通过replicaof命令形成主库和从库的关系，之后按照三个阶段完成数据的第一次同步。

例如，现在有实例1和实例2，ip1为172.16.19.3，ip2为172.16.19.5，在实例2上运行如下代码后，实例2就变成了实例1的从库，并从实例1上复制数据。

```
replicaof 172.16.19.5 6379
```

接下来就是第一次同步的三个阶段了：

![image-20250224165914165](https://lyb-1305354270.cos.ap-beijing.myqcloud.com/image-20250224165914165.png)

### 主从级联模式分担全量复制时的主库压力

通过分析主从库之间第一次数据同步的过程，可以看到一次全量复制中，对于主库来说，需要完成两个耗时的操作：生成RDB文件和传输RDB文件。

如果从库数量很多，而且都要和主库进行全量复制的话，就会导致主库忙于fork子进程生成RDB文件，进行数据全量同步。fork这个操作会阻塞主进程处理正常请求，从而导致主库响应变慢。此外，传输RDB文件也会占用主库的网络带宽，同样给主库资源使用带来压力。

那么，"主-从-从"模式就可以用来分担主库的压力：

简单来说，我们可以在部署主从集群的时候，手动选择一个从库（比如内存资源配置较高的从库），用于级联其他的从库。然后，我们可以再选择一些从库（例如三分之一的从库），在这些从库上下命令，让它们和刚才所选的从库，建立起主从关系。

这样一来，这些从库就会知道，在进行同步时，不用再和主库进行交互了，只要和级联的从库进行写操作同步就好了，这样就可以减轻主库上面的压力。

![image-20250224171312931](https://lyb-1305354270.cos.ap-beijing.myqcloud.com/image-20250224171312931.png)

那么，一旦主从库完成了全量复制，它们之间就会一直维护着一个网络连接，主库会通过这个连接将后续收到的命令操作再同步给从库，这个过程也称为`基于长连接的命令传播`，可以避免频繁建立连接的开销。

但是这个过程中存在着风险点，最常见的就是`网络断连或阻塞`，如果发生这种情况，主从库之间就无法进行命令传播了，那么客户端就可能从从库读到旧数据。

### 主从库网络断了怎么办？

在Redis2.8之前，如果主从库在命令传播时出现了网络闪断，那么主从库就会重新进行一次全量复制，开销非常大。

从Redis2.8开始，网络断了之后，主从库会采用增量复制的方式继续同步，增量复制就是把主从库网络断连期间收到的命令，同步给从库。

那么增量复制的时候，主从库之间具体怎么保持同步呢？这里的奥妙在于repl_backlog_buffer这个缓冲区。

repl_backlog_buffer是一个环形缓冲区，主库会记录自己写到的位置，从库会记录自己读到的位置。那么刚开始的时候，主库和从库的读写位置在一起，这算是它们的起始位置，随着主库不断接收新的写操作，它在缓冲区的写位置会逐步偏离起始位置，这个偏移量就是master_repl_offset，主库接收的新写操作越多，这个偏移量就越大。

同样，从库在复制完写操作命令后，它在缓冲区的读位置也逐渐偏离刚才的起始位置，这个偏移量就是slave_repl_offset。正常情况下，这两个偏移量基本相等。

![image-20250224181224956](https://lyb-1305354270.cos.ap-beijing.myqcloud.com/image-20250224181224956.png)

主从库的连接恢复后，从库首先会给主库发送psync命令，并把自己当前的slave_repl_offset发送给从库，主库会判断自己的master_repl_offset和slave_repl_offset之间的差距。

在网络断连阶段，主库可能会收到新的写操作命令，所以一般来说master_repl_offset一般要大于slave_repl_offset，主库只需要把这两个偏移量之间的命令同步给从库就可以。

![image-20250224181722769](https://lyb-1305354270.cos.ap-beijing.myqcloud.com/image-20250224181722769.png)

不过，由于repl_backlog_buffer是一个环形缓冲区，所以在缓冲区写满后，主库会继续写入，此时，就会覆盖掉之前写入的操作。如果从库的读取速度比较慢，就有可能导致从库还没读取的操作就已经被主库新写的操作给覆盖了，这样就会导致主从库数据不一致。解决办法就是调整`repl_backlog_buffer`这一参数。

```
repl_backlog_buffer = 2 * (主库写入命令速度 * 操作大小 - 主从库网络传输命令速度 * 操作大小)
```

